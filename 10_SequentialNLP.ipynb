{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RL97dD9SuTiB"
   },
   "source": [
    "## <span style=\"font-family: Arial; font-weight:bold;font-size:1.25em;color:#00b3e5;\">Domain: Sequential NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9aAJ5ohuTiC"
   },
   "source": [
    "### <span style=\"font-family: Arial; font-weight:bold;font-size:1.25em;color:#00b3e5;\">Problem Description:\n",
    "<font color=darkblue>\n",
    "Generate Word Embedding and retrieve outputs of each layer with Keras based on the Classification task.\n",
    "<br>\n",
    "Word embedding are a type of word representation that allows words with similar meaning to have a similar representation.\n",
    "<br>\n",
    "It is a distributed representation for the text that is perhaps one of the key breakthroughs for the impressive performance of deep learning methods on challenging natural language processing problems.\n",
    "<br>\n",
    "We will use the IMDb dataset to learn word embedding as we train our dataset. This dataset contains 25,000 movie reviews from IMDB, labeled with a sentiment (positive or negative).\n",
    "<br>\n",
    "</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwtNlI4puTiD"
   },
   "source": [
    "### <span style=\"font-family: Arial; font-weight:bold;font-size:1.25em;color:#00b3e5;\">Dataset:\n",
    "<font color=darkblue>\n",
    "The Dataset of 25,000 movie reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, the words are indexed by their frequency in the dataset, meaning the for that has index 1 is the most frequent word. Use the first 20 words from each review to speed up training, using a max vocab size of 10,000. As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n",
    "<br>\n",
    "</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrMX8g5BuTiE"
   },
   "source": [
    "### <span style=\"font-family: Arial; font-weight:bold;font-size:1.25em;color:#00b3e5;\">Overview:\n",
    "<font color=darkblue>\n",
    "Using the IMDB dataset and generate Word Embeddings. Build a Sequential Model using Keras for the Sentiment Classification task and Report the Accuracy of the model\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbjeCBreuTiF"
   },
   "source": [
    "### <span style=\"font-family: Arial; font-weight:bold;font-size:1.25em;color:#00b3e5;\">Objective:\n",
    "<font color=darkblue>Build a Sequential Model using Keras for the Sentiment Classification task and Report the Accuracy of the model\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNzIAk3zuTiG"
   },
   "source": [
    "### <span style=\"font-family: Arial; font-weight:bold;font-size:1.25em;color:#00b3e5;\">Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YD595UY9uTiI",
    "outputId": "fd7a8402-fbc0-4449-f95a-fe93b9f0c6c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy Version :  1.19.4\n",
      "Pandas Version :  1.1.3\n",
      "Matplotlib Version :  3.2.2\n"
     ]
    }
   ],
   "source": [
    "print('Numpy Version : ', np.__version__)\n",
    "print('Pandas Version : ', pd.__version__)\n",
    "print('Matplotlib Version : ', matplotlib.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"font-family: Arial; font-weight:bold;font-size:1.25em;color:#00b3e5;\">Load IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"font-family: Arial; font-weight:bold;font-size:1.25em;color:#00b3e5;\">Import test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000 #vocab size\n",
    "maxlen = 500  #number of word used from each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "C:\\Users\\rloganat\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "C:\\Users\\rloganat\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "# vocab_size is no.of words to consider from the dataset, ordering based on frequency.\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,) (25000,) (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: \n",
      "9998\n",
      "9951\n"
     ]
    }
   ],
   "source": [
    "# Summarize number of words\n",
    "print(\"Number of words: \")\n",
    "print(len(np.unique(np.hstack(X_train))))\n",
    "print(len(np.unique(np.hstack(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: \n",
      "(50000,)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "# summarize size\n",
    "print(\"Training data: \")\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: [0 1]\n",
      "Number of unique words: 9998\n"
     ]
    }
   ],
   "source": [
    "# Summarize number of classes\n",
    "print(\"Categories:\", np.unique(y))\n",
    "print(\"Number of unique words:\", len(np.unique(np.hstack(X))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"font-family: Arial; font-weight:bold;font-size:1.25em;color:#00b3e5;\">Get the word index and then Create a key-value pair for word and word_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary mapping words to an integer index\n",
    "word_index = imdb.get_word_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first indices are reserved\n",
    "word_index = {k: (v + 3) for k, v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0  Negative Review\n",
      "[1, 778, 128, 74, 12, 630, 163, 15, 4, 1766, 7982, 1051, 2, 32, 85, 156, 45, 40, 148, 139, 121, 664, 665, 10, 10, 1361, 173, 4, 749, 2, 16, 3804, 8, 4, 226, 65, 12, 43, 127, 24, 2, 10, 10]\n",
      "<START> begins better than it ends funny that the russian submarine crew <UNK> all other actors it's like those scenes where documentary shots br br spoiler part the message <UNK> was contrary to the whole story it just does not <UNK> br br\n"
     ]
    }
   ],
   "source": [
    "n=5\n",
    "if y[n]==1:\n",
    "  print(\"Label:\", y[n], \" Positive Review\")\n",
    "else:\n",
    "  print(\"Label:\", y[n], \" Negative Review\")\n",
    "print(X[n])\n",
    " \n",
    "decoded = decode_review(X[n])\n",
    "print(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1  Positive Review\n",
      "[1, 18, 1450, 9, 6, 2, 6532, 8341, 2, 10, 10, 12, 9, 1167, 8, 1582, 89, 111, 87, 1956, 28, 2, 1450, 11, 5738, 2868, 690, 2, 346, 21, 3210, 4414, 5, 422, 102, 4, 6541, 22, 167, 2431, 2, 301, 44, 1450, 10, 10, 12, 2564, 4, 3537, 4440, 2, 467, 4, 890, 2690, 6121, 11, 2, 19, 2, 11, 129, 3062, 4099, 5, 2, 2, 2, 5, 8159, 33, 314, 316, 11, 4, 182, 47, 107, 2, 27, 205, 5, 1450, 10, 10, 1450, 9, 210, 3445, 19, 119, 5, 883, 5, 1450, 7824, 9772, 63, 9, 9048, 2, 6978, 9, 6, 1594, 7, 346, 108, 400, 7352, 39, 3070, 1020, 907, 39, 32, 120, 4, 182, 11, 257, 75, 413, 1081, 19, 31, 7, 4, 543, 7, 641, 891, 2, 5, 19, 4, 2, 7, 32, 2088, 2, 2511, 5, 4260, 37, 32, 855, 11, 119, 11, 94, 111, 9809, 5, 5592, 11, 49, 7, 4, 2, 6978, 75, 26, 4, 4684, 7, 4, 2076, 3267, 7, 4, 5082, 15, 485, 8, 4181, 602, 2, 5, 382, 649, 40, 18, 2, 5, 2, 2, 11, 4, 890, 7, 2, 11, 4, 636, 22, 42, 18, 2, 2, 5, 2, 2, 17, 6, 428, 430, 5, 6, 4863, 250, 625, 1665, 2667, 883, 526, 34, 2, 2, 778, 23, 2, 852, 2, 13, 62, 1470, 83, 14, 2385, 4, 1984, 346, 22, 34, 7267, 1171, 2, 11, 3458, 2, 31, 430, 2, 27, 483, 46, 8, 160, 430, 2, 7, 2117, 2076, 2, 2254, 2, 8, 683, 115, 4354, 15, 4, 3258, 7, 27, 602, 127, 24, 391, 785, 10, 10, 49, 7, 4, 7428, 26, 3049, 5, 60, 465, 11, 2345, 2, 5, 2, 2, 2, 7434, 2, 2, 2, 2, 480, 735, 2305, 2016, 18, 2907, 368, 7, 1698, 9, 686, 780, 707, 452, 37, 47, 8, 157, 17, 6, 2, 11, 6, 3092, 3243, 8, 990, 18, 2, 121, 59, 4214, 41, 896, 175, 1972, 159, 59, 271, 8, 157, 31, 7, 91, 906, 5, 371, 5448, 108, 9, 273, 6172, 2, 34, 2402, 2, 2, 2, 5, 2, 2, 1001, 323, 17, 107, 185, 84, 18, 37, 119, 100, 28, 575, 50, 71, 4, 4931, 7, 12, 21, 12, 16, 605, 346, 688, 8, 3680, 5, 2, 15, 26, 984, 2587, 5, 4, 543, 7, 119, 5, 641, 9, 57, 1401, 160, 31, 15, 66, 188, 8, 72, 16, 2, 398, 5, 526, 34, 2, 2, 1184, 8020, 2, 6689, 4798, 5, 2, 2, 2, 47, 2917, 46, 7, 119, 19, 27, 322, 4798, 21, 54, 29, 9, 1623, 8, 563, 19, 4, 307, 4727, 4, 6425, 1636, 39, 27, 5078, 1042, 2957, 10, 10, 13, 70, 140, 23, 2, 23, 32, 3070, 392, 5786, 13, 40, 49, 7, 98, 55, 76, 4, 409, 421, 815, 5, 382, 80, 30, 1551, 515, 21, 444, 13, 244, 55, 1264, 15, 13, 1247, 4, 288, 5, 13, 124, 15, 13, 80, 994, 8, 61, 514, 108, 174, 5, 174, 36, 26, 273, 6172, 2, 15, 207, 1046, 460, 2, 2, 526, 34, 3728, 4913, 15, 2290, 4, 1213, 7, 31, 7, 4, 2, 5, 2, 349, 126, 735, 2, 3802, 9256, 4, 167, 7, 2, 37, 62, 607, 31, 3206, 649, 9256, 82, 526, 2, 2, 11, 63, 6, 2610, 655, 2212, 1208, 3504, 39, 8944, 1001, 5644, 4, 543, 23, 41, 205, 3760, 4, 544, 120, 11, 785, 19, 4, 1185, 1191, 2, 3198, 9, 31, 7, 4, 91, 728, 5, 367, 19, 642, 2, 34, 5223, 5, 4629, 2, 19, 37, 334, 1230, 2, 9, 61, 1557, 514, 11, 199, 3153, 298, 9349, 870, 23, 4, 2, 2, 11, 4, 1547, 5, 1230, 11, 1450, 2, 103, 5501, 2, 2, 4, 113, 187, 98, 19, 4, 275, 1901, 137, 2, 203, 135, 61, 2, 619, 5, 641, 61, 8534, 9, 1927, 2, 109, 80, 169, 46, 15, 518, 60, 4, 91, 2, 5, 4507, 6776, 3619, 62, 24, 339, 6, 6776, 6739, 399, 4, 355, 183, 11, 6, 2189, 704]\n",
      "<START> for paris is a <UNK> feast ernest <UNK> br br it is impossible to count how many great talents have <UNK> paris in paintings novels songs <UNK> short but unforgettable quotes and yes movies the celebrated film director max <UNK> said about paris br br it offered the shining wet <UNK> under the street lights breakfast in <UNK> with <UNK> in your glass coffee and <UNK> <UNK> <UNK> and prostitutes at night everyone in the world has two <UNK> his own and paris br br paris is always associated with love and romance and paris je t'aime which is subtitled <UNK> romances is a collection of short films often sketches from 18 talented directors from all over the world in each we become familiar with one of the city of light 20 <UNK> and with the <UNK> of all ages <UNK> colors and backgrounds who all deal in love in its many variations and stages in some of the <UNK> romances we are the witnesses of the unexpected encounters of the strangers that lead to instant interest <UNK> and perhaps relationship like for <UNK> and <UNK> <UNK> in the street of <UNK> in the opening film or for <UNK> <UNK> and <UNK> <UNK> as a white boy and a muslim girl whose cross cultural romance directed by <UNK> <UNK> begins on <UNK> de <UNK> i would include into this category the humorous short film by gus van <UNK> in le <UNK> one boy <UNK> his heart out to another boy <UNK> of sudden unexpected <UNK> asking <UNK> to call never realizing that the object of his interest does not understand french br br some of the vignettes are poignant and even dark in walter <UNK> and <UNK> <UNK> <UNK> du <UNK> <UNK> <UNK> <UNK> amazing oscar nominated debut for maria full of grace is single working class mother who has to work as a <UNK> in a wealthy neighborhood to pay for <UNK> where she drops her baby every morning before she goes to work one of most memorable and truly heartbreaking films is place des <UNK> by oliver <UNK> <UNK> <UNK> and <UNK> <UNK> co star as two young people for who love could have happened there were the promises of it but it was cut short due to hatred and <UNK> that are present everywhere and the city of love and light is no exception another one that really got to me was <UNK> written and directed by <UNK> <UNK> starring sergio <UNK> miranda richardson and <UNK> <UNK> <UNK> has fallen out of love with his wife richardson but when he is ready to leave with the beautiful mistress the devastating news from his wife's doctor arrives br br i can go on <UNK> on all 18 small gems i like some of them very much the others felt weak and perhaps will be forgotten soon but overall i am very glad that i bought the dvd and i know that i will return to my favorite films again and again they are place des <UNK> that i've mentioned already <UNK> <UNK> directed by wes craven that involves the ghost of one of the <UNK> and <UNK> men ever oscar <UNK> alexander payne the director of <UNK> who would save one troubled relationship payne also directed <UNK> <UNK> in which a lonely middle aged post worker from denver co explores the city on her own providing the voice over in french with the heavy accent <UNK> entry is one of the most moving and along with hilarious <UNK> by joel and ethan <UNK> with who else steve <UNK> is my absolute favorite in both shorts american tourists sit on the <UNK> <UNK> in the park and steve in paris <UNK> after visiting <UNK> <UNK> the life around them with the different results while <UNK> may say my <UNK> sad and light my sorrow is bright <UNK> character will find out that sometimes even the most <UNK> and useful tourist guide would not help a tourist avoiding doing the wrong things in a foreign country\n"
     ]
    }
   ],
   "source": [
    "n=11509\n",
    "if y[n]==1:\n",
    "  print(\"Label:\", y[n], \" Positive Review\")\n",
    "else:\n",
    "  print(\"Label:\", y[n], \" Negative Review\")\n",
    "print(X[n])\n",
    " \n",
    "decoded = decode_review(X[n])\n",
    "print(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1  Positive Review\n",
      "[1, 45, 6, 902, 14, 20, 9, 38, 254, 8, 79, 129, 957, 23, 11, 4, 178, 13, 258, 12, 143, 6, 1281, 374, 6409, 5, 12, 16, 434, 290, 12, 14, 9, 209, 6, 824, 4, 118, 22, 93, 315, 4, 1751, 2155, 999, 5, 4, 1885, 22, 7, 4, 3814, 4796, 167, 1265, 2, 93, 389, 108, 44, 4, 3494, 5, 19, 1601, 1662, 29, 1075, 6, 2447, 787, 6929, 4, 2, 7, 4, 999, 10, 10, 6151, 185, 5, 6611, 3064, 28, 6, 389, 1175, 200, 98, 5, 36, 339, 97, 14, 20, 6, 389, 883, 2, 2, 9, 1047, 5, 9119, 137, 2, 988, 9, 8368, 5, 4590, 125, 4, 3921, 200, 4, 109, 2126, 31, 7, 4, 91, 878, 21, 11, 4, 130, 6586, 1519, 23, 22, 10, 10, 1601, 1662, 9, 4, 91, 1796, 1152, 1751, 2155, 22, 207, 110, 2, 1077, 4, 2, 5, 7255, 3120, 8, 471, 4, 2, 2059, 121, 988, 5, 2, 412, 83, 6, 5061, 4, 2, 7, 4, 3494, 26, 115, 3714, 11, 192, 507, 9915, 8, 4, 22, 21, 17, 2, 2, 4, 22, 17, 6, 1796, 1152, 2447, 787, 4, 119, 200, 4, 105, 166, 4, 904, 306, 329, 2494, 12, 166, 4, 22, 2272, 5, 2, 10, 10, 1601, 1662, 9, 4, 2, 3603, 7, 4, 1751, 2155, 999, 48, 64, 53, 84, 100, 67, 12]\n",
      "<START> it's a shame this movie is so hard to get your hands on in the us i found it through a rare video dealer and it was certainly worth it this is without a doubt the best film made during the pre code era and the finest film of the 1930s masterful director frank <UNK> made wonderful films about the depression and with man's castle he created a fairy tale amidst the <UNK> of the era br br loretta young and spencer tracy have a wonderful chemistry between them and they help make this movie a wonderful romance <UNK> <UNK> is sweet and hopeful while <UNK> bill is gruff and closed off the dynamic between the character creates one of the most difficult but in the end rewarding relationships on film br br man's castle is the most soft focus pre code film i've seen <UNK> uses the <UNK> and dreamy technique to turn the <UNK> village where bill and <UNK> live into a palace the <UNK> of the depression are never ignored in fact they're integral to the film but as <UNK> <UNK> the film as a soft focus fairy tale the love between the characters makes the situation seem less harsh it makes the film warm and <UNK> br br man's castle is the <UNK> achievement of the pre code era if only more people could see it\n"
     ]
    }
   ],
   "source": [
    "n=13509\n",
    "if y[n]==1:\n",
    "  print(\"Label:\", y[n], \" Positive Review\")\n",
    "else:\n",
    "  print(\"Label:\", y[n], \" Negative Review\")\n",
    "print(X[n])\n",
    " \n",
    "decoded = decode_review(X[n])\n",
    "print(decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family: Arial; font-weight:bold;font-size:1.25em;color:#00b3e5;\">Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, value=word_index[\"<PAD>\"], padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, value=word_index[\"<PAD>\"], padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 500)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0]), len(X_train[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"font-family: Arial; font-weight:bold;font-size:1.25em;color:#00b3e5;\">Build a Sequential Model using Keras for the Sentiment Classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family: Arial; font-weight:bold;font-size:1.25em;color:#00b3e5;\">Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vecor_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seq = Sequential()\n",
    "model_seq.add(Embedding(vocab_size, embedding_vecor_length, input_length=maxlen))\n",
    "model_seq.add(Flatten())\n",
    "model_seq.add(Dense(250, activation='relu'))\n",
    "model_seq.add(Dense(1, activation='sigmoid'))\n",
    "model_seq.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 20)           200000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 250)               2500250   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 2,700,501\n",
      "Trainable params: 2,700,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model_seq.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "196/196 - 5s - loss: 0.4964 - accuracy: 0.7231\n",
      "Epoch 2/5\n",
      "196/196 - 5s - loss: 0.1811 - accuracy: 0.9322\n",
      "Epoch 3/5\n",
      "196/196 - 5s - loss: 0.0503 - accuracy: 0.9866\n",
      "Epoch 4/5\n",
      "196/196 - 6s - loss: 0.0099 - accuracy: 0.9986\n",
      "Epoch 5/5\n",
      "196/196 - 6s - loss: 0.0023 - accuracy: 0.9998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26da77d31f0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit the model\n",
    "model_seq.fit(X_train, y_train, epochs=5, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.59%\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "scores_seq = model_seq.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (scores_seq[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family: Arial; font-weight:bold;font-size:1.25em;color:#00b3e5;\">LSTM for Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seq_lstm = Sequential()\n",
    "model_seq_lstm.add(Embedding(vocab_size, embedding_vecor_length, input_length=maxlen))\n",
    "model_seq_lstm.add(LSTM(100))\n",
    "model_seq_lstm.add(Dense(1, activation='sigmoid'))\n",
    "model_seq_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 20)           200000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 248,501\n",
      "Trainable params: 248,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model_seq_lstm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "196/196 - 200s - loss: 0.6929 - accuracy: 0.5030\n",
      "Epoch 2/3\n",
      "196/196 - 206s - loss: 0.6967 - accuracy: 0.5177\n",
      "Epoch 3/3\n",
      "196/196 - 213s - loss: 0.6741 - accuracy: 0.5287\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26dab818490>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit the model\n",
    "model_seq_lstm.fit(X_train, y_train, epochs=3, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 51.24%\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "scores_seq_lstm = model_seq_lstm.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (scores_seq_lstm[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family: Arial; font-weight:bold;font-size:1.25em;color:#00b3e5;\">LSTM for Sequence Classification with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seq_lstm_dropout = Sequential()\n",
    "model_seq_lstm_dropout.add(Embedding(vocab_size, embedding_vecor_length, input_length=maxlen))\n",
    "model_seq_lstm_dropout.add(Dropout(0.2))\n",
    "model_seq_lstm_dropout.add(LSTM(100))\n",
    "model_seq_lstm_dropout.add(Dropout(0.2))\n",
    "model_seq_lstm_dropout.add(Dense(1, activation='sigmoid'))\n",
    "model_seq_lstm_dropout.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 20)           200000    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 500, 20)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 248,501\n",
      "Trainable params: 248,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model_seq_lstm_dropout.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "196/196 - 215s - loss: 0.6933 - accuracy: 0.5031\n",
      "Epoch 2/3\n",
      "196/196 - 212s - loss: 0.6918 - accuracy: 0.5080\n",
      "Epoch 3/3\n",
      "196/196 - 203s - loss: 0.6886 - accuracy: 0.5192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26da6a076a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit the model\n",
    "model_seq_lstm_dropout.fit(X_train, y_train, epochs=3, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.44%\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "scores_seq_lstm_dropout = model_seq_lstm_dropout.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (scores_seq_lstm_dropout[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family: Arial; font-weight:bold;font-size:1.25em;color:#00b3e5;\">precise LSTM for Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seq_lstm_precise = Sequential()\n",
    "model_seq_lstm_precise.add(Embedding(vocab_size, embedding_vecor_length, input_length=maxlen))\n",
    "model_seq_lstm_precise.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_seq_lstm_precise.add(Dense(1, activation='sigmoid'))\n",
    "model_seq_lstm_precise.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 20)           200000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 248,501\n",
      "Trainable params: 248,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model_seq_lstm_precise.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "196/196 - 401s - loss: 0.6937 - accuracy: 0.5038\n",
      "Epoch 2/3\n",
      "196/196 - 405s - loss: 0.6908 - accuracy: 0.5142\n",
      "Epoch 3/3\n",
      "196/196 - 402s - loss: 0.6858 - accuracy: 0.5242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26da90628e0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit the model\n",
    "model_seq_lstm_precise.fit(X_train, y_train, epochs=3, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 51.49%\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "scores_seq_lstm_precise = model_seq_lstm_precise.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (scores_seq_lstm_precise[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family: Arial; font-weight:bold;font-size:1.25em;color:#00b3e5;\">CNN and LSTM for Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seq_lstm_cnn = Sequential()\n",
    "model_seq_lstm_cnn.add(Embedding(vocab_size, embedding_vecor_length, input_length=maxlen))\n",
    "model_seq_lstm_cnn.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model_seq_lstm_cnn.add(LSTM(100))\n",
    "model_seq_lstm_cnn.add(Dense(1, activation='sigmoid'))\n",
    "model_seq_lstm_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 500, 20)           200000    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 500, 32)           1952      \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 255,253\n",
      "Trainable params: 255,253\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model_seq_lstm_cnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "196/196 - 180s - loss: 0.6978 - accuracy: 0.5034\n",
      "Epoch 2/3\n",
      "196/196 - 204s - loss: 0.6813 - accuracy: 0.5250\n",
      "Epoch 3/3\n",
      "196/196 - 205s - loss: 0.6695 - accuracy: 0.5322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26dd09f9b80>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit the model\n",
    "model_seq_lstm_cnn.fit(X_train, y_train, epochs=3, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 51.32%\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "scores_seq_lstm_cnn = model_seq_lstm_cnn.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (scores_seq_lstm_cnn[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"font-family: Arial; font-weight:bold;font-size:1.25em;color:#00b3e5;\">Retrieve the output of each layer in Keras for a given single test sample from the trained model you built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with a Sequential model\n",
    "layer1_output = K.function([model_seq.layers[0].input], [model_seq.layers[1].output])\n",
    "layer2_output = K.function([model_seq.layers[0].input], [model_seq.layers[2].output])\n",
    "layer3_output = K.function([model_seq.layers[0].input], [model_seq.layers[3].output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = layer1_output([X_test][0])\n",
    "layer2 = layer2_output([X_test][0])\n",
    "layer3 = layer3_output([X_test][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.01447504,  0.00384187,  0.00748494, ...,  0.00550764,\n",
       "         -0.00560445,  0.00756306],\n",
       "        [-0.01447504,  0.00384187,  0.00748494, ...,  0.00550764,\n",
       "         -0.00560445,  0.00756306],\n",
       "        [-0.03957103,  0.02544248, -0.00233473, ...,  0.00263179,\n",
       "         -0.01730805,  0.04146944],\n",
       "        ...,\n",
       "        [-0.01447504,  0.00384187,  0.00748494, ...,  0.00550764,\n",
       "         -0.00560445,  0.00756306],\n",
       "        [-0.01447504,  0.00384187,  0.00748494, ...,  0.00550764,\n",
       "         -0.00560445,  0.00756306],\n",
       "        [-0.01447504,  0.00384187,  0.00748494, ...,  0.00550764,\n",
       "         -0.00560445,  0.00756306]], dtype=float32)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.11262846, 0.17361796, 0.15848213, ..., 0.21624184, 0.28417572,\n",
       "         0.40211496],\n",
       "        [0.6366247 , 0.6599688 , 0.7278754 , ..., 1.0163685 , 0.        ,\n",
       "         0.        ],\n",
       "        [0.41450405, 0.41564944, 0.37862363, ..., 0.18982676, 0.24773087,\n",
       "         0.08817422],\n",
       "        ...,\n",
       "        [0.10430388, 0.12525654, 0.13948679, ..., 0.1437923 , 0.25996444,\n",
       "         0.38182613],\n",
       "        [0.21796092, 0.2222244 , 0.21280958, ..., 0.22931527, 0.16754338,\n",
       "         0.26743484],\n",
       "        [0.32403067, 0.37360978, 0.3495034 , ..., 0.34490663, 0.11185843,\n",
       "         0.20344909]], dtype=float32)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.02823198],\n",
       "        [0.99999964],\n",
       "        [0.98118687],\n",
       "        ...,\n",
       "        [0.0104104 ],\n",
       "        [0.27146804],\n",
       "        [0.9827529 ]], dtype=float32)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVZmMZgEuTi2"
   },
   "source": [
    "#### <span style=\"font-family: Arial; font-weight:bold;font-size:1.25em;color:#00b3e5;\">Conclusion :\n",
    "<font color=darkblue>\n",
    "Goal is to build a Sequential Model and Report the Accuracy of the model. We loaded the IMDB daa set using Keras, generated word_index, created Sequential modeland reported accuracy. Output of each layer from Sequential model was displayed.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWT75i-suTi3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "cv_proj2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
